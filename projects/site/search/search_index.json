{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Elastic Machine Learning Whitelist Guide","text":"<p>Welcome to the Elastic Machine Learning Whitelist Guide! This guide will walk you through creating and using a trained machine learning model to predict whether alerts in Elastic should be whitelisted or not.</p> <p></p>"},{"location":"#steps","title":"Steps","text":""},{"location":"#get-the-data","title":"Get the data","text":"<p>The first step to training a machine learning model is to get the data that you need, for this specific project we will download the data that we need from Elastic. We can do that by going to Elastic&gt; Click on the hamburger icon at the top left of the screen&gt; Under the Analytics section click on Discover.</p> <p>Now that we are on the Discover page we need to switch our index to our siem index after this, we can then begin to download the necessary data, first lets select a date by clicking on the time picker then, we can click on Share at the top right of the screen, and click CSV Reports&gt; Generate CSV. We want to download as much data as we need and we can do so by repeating the steps above as there is a limit on how much data you can download at once.</p>"},{"location":"#getting-the-data-ready-transform-clean","title":"Getting the data ready: Transform &amp; Clean","text":"<p>Now that we have our data we want to get our data ready for training, we are going to need to concatenate all those csv files that we previously downloaded and then we want to clean up the data a bit, here is how we do that.</p> <p>This code will request the list of features you would like your model to have and, put those features in parenthesis and add a comma at the end of each feature so that you can easily copy and paste the text into the code below. <pre><code>words_input = input(\"Please paste the list of words: \")\n\nwords = words_input.split()\n\nprocessed_words = \"\\n\".join([f\"'{word}',\" for word in words])\n\nprint(processed_words)\n</code></pre> Now that we have the features we want our model to be trained with lets concatenate those csv files we previously downloaded (make sure to rename these files accordingly e.g. file1.csv, file2.csv, file3.csv) and then remove every column that is not in our feature list.</p> <pre><code>import pandas as pd\nimport numpy as np\n\nfiles = ['file1.csv', 'file2.csv', 'file3.csv', 'file4.csv', 'file5.csv', 'file6.csv', 'file7.csv', 'file8.csv']\n\ndfs = []\nfor f in files:\n    df = pd.read_csv(f, on_bad_lines='warn')\n    print(f\"File: {f}, Columns: {df.columns.tolist()}\")\n    dfs.append(df)\n\nresult = pd.concat(dfs, ignore_index=True)\n\ncols_to_keep = [\n    'kibana.alert.rule.name',\n    'user.name',\n    'user.domain',\n    'host.name',\n    'process.name',\n    'event.category',\n    'source.ip',\n    'source.port',\n    'destination.port',\n    'dns.question.name',\n    'dns.question.type',\n    'file.name',\n    'file.path',\n    'dll.name',\n    'dll.path',\n    'process.parent.name',\n    'process.executable',\n    'process.working_directory',\n    'process.args',\n    'process.hash.sha256',\n    'dll.hash.sha256',\n    'signal.reason'\n]\n\ncols_to_keep = [col for col in cols_to_keep if col in result.columns]\n\nresult = result[cols_to_keep]\n\nresult = result.replace('-', np.nan)\n\nresult.to_csv('siem.csv', index=False, header=True)\n</code></pre>"},{"location":"#elastic-model-trainng","title":"Elastic Model Trainng","text":"<p>To import our data into elastic Click on the hamburger icon at the top left of the screen&gt; Then on Machine Learning&gt; and then on File&gt; Import the appropriate csv file that was concatenated and cleaned. Now that we have our data ready to go, lets train our model. Click on the hamburger icon at the top left of the screen&gt; Under Analytics click on Machine Learning&gt; Under Data Frame Analytics click on Jobs&gt; Then click Create job As we create our ML mode make sure to set \"Feature Importance Values to the amount of features we have.</p>"},{"location":"#elastic-pipeline","title":"Elastic Pipeline","text":"<p>Now that we have our model created we can now create the pipeline that will run our model againts our elastic siem logs. See code below, you can copy and paste the code in the Elastic Dev Console (Make sure to configure Pipeline Name, Description, Mode ID, and Field Map)</p> <pre><code>PUT _ingest/pipeline/ml-mitre-ta0002-execution-pipeline \n\n{ \n\n  \"description\": \"Inference pipeline for ml-mitre-ta0002-execution model\", \n\n  \"processors\": [ \n\n    { \n\n      \"inference\": { \n\n        \"model_id\": \"ml-mitre-ta0002-execution-model-1686766767712\", \n\n        \"inference_config\": { \n\n          \"classification\": {} \n\n        }, \n\n        \"field_map\": { \n\n          \"user.name\": \"user_name\", \n\n          \"host.name\": \"host_name\", \n\n          \"process.name\": \"process_name\", \n\n          \"process.parent.name\": \"process_parent_name\", \n\n          \"process.executable\": \"process_executable\", \n\n          \"process.parent.executable\": \"process_parent_executable\", \n\n          \"process.working_directory\": \"process_working_directory\", \n\n          \"process.parent.working_directory\": \"process_parent_working_directory\", \n\n          \"process.args\": \"process_args\", \n\n          \"process.parent.args\": \"process_parent_args\", \n\n          \"process.entity_id\": \"process_entity_id\", \n\n          \"process.parent.entity_id\": \"process_parent_entity_id\", \n\n          \"host.os.type\": \"host_os_type\" \n\n        } \n\n      } \n\n    }, \n\n    { \n\n      \"script\": { \n\n        \"source\": \"\"\" \n\n          if (ctx.containsKey('user') &amp;&amp; ctx.user.containsKey('id') &amp;&amp; ctx.user.id instanceof String) { \n\n            try { \n\n              ctx.user.id = Long.parseLong(ctx.user.id); \n\n            } catch (NumberFormatException e) { \n\n              ctx.user.id = null; \n\n            } \n\n          } \n\n        \"\"\" \n\n      } \n\n    }, \n\n    { \n\n      \"set\": { \n\n        \"field\": \"whitelist\", \n\n        \"value\": \"{{ml.inference.whitelist}}\" \n\n      } \n\n    }, \n\n    { \n\n      \"script\": { \n\n        \"source\": \"\"\" \n\n          if (ctx.whitelist.equals(\"1\")) { \n\n            ctx.whitelist = true; \n\n          } else if (ctx.whitelist.equals(\"0\")) { \n\n            ctx.whitelist = false; \n\n          } \n\n        \"\"\" \n\n      } \n\n    } \n\n  ] \n\n} \n</code></pre>"},{"location":"#creating-azure-function","title":"Creating Azure Function","text":"<p>Now that we have the pipeline and the model all done, next we need to set up an Azure Function so that we can automate the process of running our models againts our siem logs. Here is how we do that...</p> <ol> <li>Activate your role in azure</li> <li>Download the \"Azure Account\", \"Azure Functions\", and \"Azure Resources\" extensions in VSCode.</li> <li>Click on the Azure Icon on the left side bar in VSCode.</li> <li>Sign into your Azure account.</li> <li>Create an Azure Function by clicking on the Subscription of your choice, then right click on Function App, then click Create Function App in Azure</li> <li>Enter a Globally Unique name for you Azure Function.</li> <li>Select the appropriate Coding Language</li> <li>Select a Location</li> <li>Select a Resource Group</li> <li>Now you can insert your code into the .py file called init.py</li> </ol>"}]}